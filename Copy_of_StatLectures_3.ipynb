{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kiboiWanjohi/AirBnB_clone/blob/main/Copy_of_StatLectures_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color='red'>$\\Huge{\\textbf{School of Computational Techniques}}$\n",
        "$\\Huge{\\textbf{for Physics Students in Kenya}}$\n",
        "\n",
        "$\\Huge{\\text{Introduction to Statistical Data Analysis}}$\n",
        "\n",
        "Christina Agapopoulou,\n",
        "Kiplabat Tarus"
      ],
      "metadata": {
        "id": "C_UP8XdOT_Xf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The Central Limit Theorem\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "The central limit theorem states that, in the limit of large sampling, no matter what the distribution of the samples, their means are distributed by a Gaussian whose mean is the mean of the distribution and whose standard deviation decreases as $1/ \\sqrt{n}$.\n",
        "\n",
        "What does this tell us in practice? It tells us that if we measure a quantity over and over many times, its mean will be a Gaussian distributed variable centered around the true value - even if the quantity is not distributed gaussianly! And the more measurements we take, the less fluctuations we will get  when averaging.\n",
        "\n",
        "Let's see how this works in practice.\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "LpoFOa72YH4L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define how many experiments we're going to generate, and how many events each will have\n",
        "n = 1\n",
        "sample_size = 1000\n",
        "\n",
        "# Generate random samples from a normal distribution, each sample has 10000 events\n",
        "samples = np.random.normal(loc=0, scale=1, size=(n, sample_size))\n",
        "print(\"Shape of my samples array:\", np.shape(samples))\n",
        "\n",
        "# Calculate the mean and standard deviation for the array of sample means\n",
        "sample_mean = np.mean(samples[0])\n",
        "sample_std = np.std(samples[0])\n",
        "\n",
        "print(\"Mean of 1st sample: %.3f\" % sample_mean)\n",
        "print(\"Standard deviation of 1st sample: %.3f\"% sample_std)\n",
        "\n",
        "\n",
        "plt.hist(samples[0], bins=50, edgecolor='black', density=True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "P40AzUvHXSWj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "We have generated a random sample, of 1000 events, distributed gaussianly. How would we go about checking that the CLT is valid?\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "Vx7-18RVYp6s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This is the array of the means of each sample\n",
        "sample_means = np.mean(samples, axis=1)\n",
        "print(\"Shape of my means-of-samples array:\", np.shape(sample_means))\n",
        "# Calculate the mean and standard deviation for the array of sample means\n",
        "samples_mean = np.mean(sample_means)\n",
        "samples_std = np.std(sample_means)\n",
        "print(\"Mean of sample means: %.3f\" % samples_mean)\n",
        "print(\"Standard deviation of sample means %.3f\"% samples_std)\n",
        "\n",
        "# Plot the histogram of sample means\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.hist(sample_means, bins=50, edgecolor='black', density=True)\n",
        "# Add labels and legend\n",
        "plt.title('Histogram of Sample Means of a Normal Distribution')\n",
        "plt.xlabel('Sample Mean')\n",
        "plt.ylabel('Frequency')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "iU1i9NvyYLma"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "It feels natural that the CLT holds for Gaussian distributions. But let's check that it really works for other types of distributions as well!\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "IOJ-kSEFknJo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define how many experiments we're going to generate, and how many events each will have\n",
        "n = 1\n",
        "sample_size = 10000\n",
        "\n",
        "# Generate random samples from a normal distribution, each sample has 10000 events\n",
        "samples = np.random.exponential(2, size=(n, sample_size))\n",
        "\n",
        "plt.hist(samples[0], bins=15, edgecolor='black', density=True)\n",
        "# Calculate the mean and standard deviation for the array of sample means\n",
        "sample_mean = np.mean(samples[0])\n",
        "sample_std = np.std(samples[0])\n",
        "\n",
        "print(\"Mean of 1st sample: %.3f\" % sample_mean)\n",
        "print(\"Standard deviation of 1st sample: %.3f\"% sample_std)\n",
        "\n",
        "\n",
        "plt.title('Example of the poison distribution')\n",
        "plt.show()\n",
        "\n",
        "# This is the array of the means of each sample\n",
        "# axis = 1 tells us which dimension of our 2d array to average over\n",
        "sample_means = np.mean(samples, axis=1)\n",
        "print(\"Shape of my sample means distribution:\", np.shape(sample_means))\n",
        "# Calculate the mean and standard deviation for the array of sample means\n",
        "samples_mean = np.mean(sample_means)\n",
        "samples_std = np.std(sample_means)\n",
        "print(\"Mean of sample means: %.3f\" % samples_mean)\n",
        "print(\"Standard deviation of sample means %.3f\"% samples_std)\n",
        "\n",
        "# Plot the histogram of sample means\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.hist(sample_means, bins=20, edgecolor='black')\n",
        "# Add labels and legend\n",
        "plt.title('Histogram of Sample Means of a Exponential Distribution')\n",
        "plt.xlabel('Sample Mean')\n",
        "plt.ylabel('Frequency')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "1vVJQ4agi07f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "There are situations where the CLT does not work well, typically when the sample size is too small or the underlying distribution has infinite variance (such as Cauchy distribution).\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "1EDxqCdBz1XK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Show me an example of the Central Limit Theorem not working\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Number of samples to draw from the Cauchy distribution\n",
        "sample_size = 1000\n",
        "\n",
        "# Number of experiments to perform\n",
        "num_experiments = 1000\n",
        "\n",
        "# Draw samples from a Cauchy distribution\n",
        "data = np.random.standard_cauchy((num_experiments, sample_size))\n",
        "\n",
        "plt.hist(samples[0], bins=50, edgecolor='black')\n",
        "plt.title('Example of the Cauchy distribution')\n",
        "plt.show()\n",
        "\n",
        "# Compute the means of these samples\n",
        "sample_means = np.mean(data, axis=1)\n",
        "\n",
        "\n",
        "# Plot the histogram of the sample means\n",
        "plt.hist(sample_means, bins=100, range=[-10, 10], density=True, alpha=0.75, color='b', edgecolor='black')\n",
        "\n",
        "\n",
        "# Add labels and legend\n",
        "plt.title('Histogram of Sample Means of Cauchy Distribution')\n",
        "plt.xlabel('Sample Mean')\n",
        "plt.ylabel('Frequency')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "pM731FF_xQos"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "We see that the Gaussian fit doesn't manage to describe well or distribution of sample means.\n",
        "\n",
        "**Explanation**: The Cauchy distribution is known for its heavy tails and undefined variance and mean.\n",
        "In the example, we draw sample_size samples from a standard Cauchy distribution for num_experiments times.\n",
        "We then compute the mean of each set of samples.\n",
        "Finally, we plot the histogram of these sample means.\n",
        "What happens:\n",
        "Unlike distributions with finite variance, the sample means from a Cauchy distribution do not converge to a normal distribution as the sample size increases.\n",
        "Instead, they remain heavy-tailed and do not exhibit the normal distribution behavior predicted by the CLT. The CLT has its limitations.\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "F4OgNVMiz-AM"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vGb580c2dl4N"
      },
      "source": [
        "### Fitting\n",
        "\n",
        "So far in this lesson, we've been generating pseudo-datasets with known PDFs... But in real life we usually get data without knowing their PDF a-priori. Understanding how are data is distributed will help us make conclusions about them.\n",
        "\n",
        "We do this with a process that we call *fit* . Data fitting is basically us guessing a PDF for our data, and trying to determine the level of agreement between our guess and reality to find the best curve. Let's see a few examples:\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PTuAWUJ3kFwn"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Generate the data\n",
        "x = np.array([1, 2, 3, 4])\n",
        "y = 2 * x + 1 #+ np.random.randn(4)\n",
        "\n",
        "# Plot the data\n",
        "plt.scatter(x, y)\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oORfZjFekMX5"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "How do you believe the data is distributed?\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "scaRPAOJi8TO"
      },
      "outputs": [],
      "source": [
        "# Define the fitting function\n",
        "def linear_function(x, m, b):\n",
        "    return m * x + b\n",
        "\n",
        "# Generate the data\n",
        "x = np.array([1, 2, 3, 4])\n",
        "y = 2 * x + 1\n",
        "\n",
        "#Try and uncomment this next part, and see what happens when you change f\n",
        "# Inject some randomness in our data\n",
        "#f = 5\n",
        "#y = y + 5* np.random.randn(4)\n",
        "\n",
        "# Plot the data\n",
        "plt.scatter(x, y)\n",
        "\n",
        "\n",
        "# Fit the data\n",
        "from scipy.optimize import curve_fit\n",
        "\n",
        "params, _ = curve_fit(linear_function, x, y)\n",
        "\n",
        "# Plot the fitted line\n",
        "x_fit = np.linspace(0, 5, 100)\n",
        "y_fit = linear_function(x_fit, *params)\n",
        "\n",
        "plt.plot(x_fit, y_fit, color='red')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n",
        "\n",
        "print(params)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uAQ_yJXUkwVI"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "Looks like our guess was not too bad! But how can we know *quantitatively* how good or bad our fit is? We will use something a *goodness-of-fit* test called *chi2*. The chi2 is a test-statistic.\n",
        "\n",
        "The chi2 test-statistic is defined like so:\n",
        "\n",
        "$\\chi^2 = \\sum_{i=1}^{n}\\frac{(y_i - f(x_i))^2}{\\sigma_i^2}$\n",
        "\n",
        "and the smaller it is, the better our fit is.\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ovZmjDH6kt3J"
      },
      "outputs": [],
      "source": [
        "# Calculate the chi2\n",
        "chi2 = np.sum(((y - linear_function(x, *params)) / np.std(y))**2)\n",
        "print(\"Chi2: %.3f\"% chi2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aFfTsGGdlplS"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Do you think this is a good or a bad fit? And what happens if you try to inject some randomness in the data?\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t7_Ih9-AmpcR"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "We see that our fit chi2 is defined from a sum of all our measurements So the more measurements we have, the more accurate we can be in our goodness-of-fit test. Also, the more parameters our fit has, the more measurements we need to make an accurate prediction. Let's see this with an example, now looking in the gaussian distribution.\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t5ESjyFGVzFk"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from scipy.stats import norm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# Plotting script\n",
        "def plot_gaussian(axis, mu, std, nentries):\n",
        "    # Generate some data for this demonstration.\n",
        "    data = norm.rvs(mu, std, size=nentries)\n",
        "    axis.hist(data, bins=25, density=True, alpha=0.6, color='g')\n",
        "\n",
        "    # Fit a normal distribution to the data:\n",
        "    mu_fit, std_fit = norm.fit(data)\n",
        "\n",
        "    # Plot the histogram.\n",
        "    x = np.linspace(0, 20, 100)\n",
        "    p = norm.pdf(x, mu_fit, std_fit)\n",
        "    axis.plot(x, p, 'k', linewidth=2)\n",
        "    title = \"Fit results: mu = %.2f,  std = %.2f\" % (mu_fit, std_fit)\n",
        "    axis.set_title(title)\n",
        "    return\n",
        "\n",
        "\n",
        "\n",
        "# Plot the histogram.\n",
        "\n",
        "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15, 5), constrained_layout=True)\n",
        "\n",
        "# Plot the PDF.\n",
        "\n",
        "plot_gaussian(ax1, 10.0, 2.5, 10)\n",
        "\n",
        "\n",
        "## Uncomment the following 2 lines, see what happens!\n",
        "plot_gaussian(ax2, 10.0, 2.5, 100)\n",
        "plot_gaussian(ax3, 10.0, 2.5, 1000)\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WFgHj_0DneUI"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "What do you observe? Is the fit compatible with the model we used to generate the dataset? What can we do to improve it?\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gUrPLtwbhSPM"
      },
      "source": [
        "### Parameter estimation using the Maximum Likelihood method\n",
        "\n",
        "**Likelihood Function:**\n",
        "\n",
        "The likelihood function, $L(\\theta)$, measures the probability of observing the sample data given the parameter $\\theta$. For a set of independent observations, $x_1, x_2,...,x_n$, the likelihood function is defined as:\n",
        "\n",
        "$$L(\\theta) = P(x_1, x_2,...,x_n | \\theta)$$\n",
        "\n",
        "For computational reasons, it is easier to work with the logarithm of the likelihood (because it's easier to make sums than products!)\n",
        "\n",
        "$$logL(\\theta) = \\Sigma_{i=0}^{n} logP(x_i | \\theta)$$\n",
        "\n",
        "\n",
        "**Maximum Likelihood Estimation (MLE):**\n",
        "\n",
        "The goal of MLE is to find the parameter value $\\hat{\\theta}$ that maximizes the likelihood function. Mathematically, this is:\n",
        "\n",
        "$$  \\hat{\\theta} = argmax L(\\theta) $$\n",
        "\n",
        "For computational reasons, it is often preferrable to actually minimize the log-likelihood function.\n",
        "\n",
        "Let's see an example:\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yYw0IYeehDua"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from scipy.optimize import minimize\n",
        "\n",
        "# Step 1: Generate Sample Data\n",
        "true_mu = 5\n",
        "true_sigma = 2\n",
        "np.random.seed(0)\n",
        "data = np.random.normal(true_mu, true_sigma, 1000)\n",
        "\n",
        "# Step 2: Define the Negative Log-Likelihood Function\n",
        "def neg_log_likelihood(params, data):\n",
        "    mu, sigma = params[0], params[1]\n",
        "    N = len(data)\n",
        "    log_likelihood = -N/2 * np.log(2 * np.pi) - N/2 * np.log(sigma**2) - 1/(2 * sigma**2) * np.sum((data - mu)**2)\n",
        "    return -log_likelihood  # We negate because we will minimize\n",
        "\n",
        "# Step 3: Use Numerical Optimization to Find Parameters\n",
        "initial_guess = [0, 1]\n",
        "result = minimize(neg_log_likelihood, initial_guess, args=(data,), bounds=[(None, None), (1e-6, None)])\n",
        "estimated_mu, estimated_sigma = result.x\n",
        "\n",
        "print(f\"True mu: {true_mu}\")\n",
        "print(f\"True sigma: {true_sigma}\")\n",
        "print(f\"Estimated mu: {estimated_mu}\")\n",
        "print(f\"Estimated sigma: {estimated_sigma}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kg5h8Fp2byqf"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "The result looks accurate! To be sure there's nothing strange going on, it's always a good idea to plot the likelihood scan.\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fNtEeFXwbyIQ"
      },
      "outputs": [],
      "source": [
        "# Step 4: Plot the Likelihood Scan for mu\n",
        "mu_values = np.linspace(estimated_mu - 2, estimated_mu + 2, 100)\n",
        "likelihood_values_mu = [neg_log_likelihood([mu, estimated_sigma], data) for mu in mu_values]\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "\n",
        "plt.scatter(mu_values, likelihood_values_mu, label='Negative Log-Likelihood (mu)')\n",
        "plt.axvline(x=estimated_mu, color='r', linestyle='--', label='Estimated mu')\n",
        "plt.xlabel('Mu')\n",
        "plt.ylabel('Negative Log-Likelihood')\n",
        "plt.title('Likelihood Scan for Mu')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QsWOjktAcGpo"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "Try a few different distributions, and see how the likelihood parameter estimation changes when considering more un-Gaussian distributions!\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VzBHsx1lcfWt"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from scipy.optimize import minimize\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Step 1: Generate Sample Data\n",
        "true_lambda = 3.5  # True parameter for Poisson distribution\n",
        "np.random.seed(0)\n",
        "data = np.random.poisson(true_lambda, 1000)\n",
        "\n",
        "# Step 2: Define the Negative Log-Likelihood Function for Poisson Distribution\n",
        "def neg_log_likelihood_poisson(lambda_, data):\n",
        "    N = len(data)\n",
        "    log_likelihood = -N * lambda_ + np.sum(data * np.log(lambda_) - np.log(np.arange(1, data.max() + 1)).sum())\n",
        "    return -log_likelihood  # We negate because we will minimize\n",
        "\n",
        "# Step 3: Use Numerical Optimization to Find Parameter (lambda)\n",
        "initial_guess = [1.0]\n",
        "result = minimize(neg_log_likelihood_poisson, initial_guess, args=(data,), bounds=[(1e-6, None)])\n",
        "\n",
        "estimated_lambda = result.x[0]\n",
        "\n",
        "print(f\"True lambda: {true_lambda}\")\n",
        "print(f\"Estimated lambda: {estimated_lambda}\")\n",
        "\n",
        "# Step 4: Plot the Likelihood Scan for lambda\n",
        "lambda_values = np.linspace(estimated_lambda - 10, estimated_lambda + 10, 100)\n",
        "likelihood_values_lambda = [neg_log_likelihood_poisson(lambda_, data) for lambda_ in lambda_values]\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.scatter(lambda_values, likelihood_values_lambda, label='Negative Log-Likelihood (lambda)')\n",
        "plt.axvline(x=estimated_lambda, color='r', linestyle='--', label='Estimated lambda')\n",
        "plt.axvline(x=true_lambda, color='b', linestyle='--', label='True lambda')\n",
        "plt.xlabel('Lambda')\n",
        "plt.ylabel('Negative Log-Likelihood')\n",
        "plt.title('Likelihood Scan for Lambda (Poisson Distribution)')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iTUbsa0Fc2oC"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "You can see that for Poissonial distributions, the ML scan looks a bit asymmetric.\n",
        "* What happens when you increase or decrease the number of scan points?\n",
        "* What if you put an initial value that is very far from the true value?\n",
        "\n",
        "---\n",
        "\n"
      ]
    }
  ]
}